{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc9bcac5-8e9a-4201-83a5-0f0a88cd116d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Building the Agent System for Prescriptive Maintenance using the Mosaic AI Agent Framework\n",
    "\n",
    "Now that have created the Mosaic AI Tools in Unity Catalog, we will leverage the Mosaic AI Agent Framework to build, deploy and evaluate an AI agent for Prescriptive Maintenance. The Agent Framework comprises a set of tools on Databricks designed to help developers build, deploy, and evaluate production-quality AI agents like Retrieval Augmented Generation (RAG) applications. Moreover, Mosaic AI Agent Evaluation provides a platform to capture and implement human feedback, ground truth, response and request logs, LLM judge feedback, chain traces, and more.\n",
    "\n",
    "This notebook uses Mosaic AI Agent Framework ([AWS](https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/retrieval-augmented-generation)) to build an agent for Prescriptive Maintenance by defining a Pyfunc agent that has access to the Mosaic AI tools create in notebook [05.1-ai-tools-iot-turbine-prescriptive-maintenance]($./05-Generative-AI/05.1-ai-tools-iot-turbine-prescriptive-maintenance). Use this notebook to iterate on and modify the agent. For example, you could add more tools or change the system prompt.\n",
    "\n",
    " **_NOTE:_**  This notebook uses Pyfunc, however AI Agent Framework is compatible with other agent frameworks like Langchain and LlamaIndex.\n",
    "\n",
    "Thi is an high-level overview of the agent system that we will build in this demo:\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://github.com/Datastohne/demo/blob/main/agent2.png?raw=true\" width=\"900px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca373bb7-18a0-49da-b848-44c74fa0fd72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-lakehouse-iot-platform-rishabh_shastry` from the dropdown menu ([open cluster configuration](https://e2-demo-field-eng.cloud.databricks.com/#setting/clusters/0430-201512-50kngnyt/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('lakehouse-iot-platform')` or re-install the demo: `dbdemos.install('lakehouse-iot-platform')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d15e4ce-6161-4cc5-b471-0cb77fff22c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny==2.20.0 langchain==0.3.19 langgraph-checkpoint==1.0.12 langchain_core langchain-community==0.2.16 langgraph==0.2.16 pydantic langchain_databricks\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "234f15c5-09e5-477d-ac93-d581d059a614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e79835dd-1c3a-42d9-8f28-649e828e8626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import and setup\n",
    "Use `mlflow.langchain.autolog()` to set up [MLflow traces](https://docs.databricks.com/en/mlflow/mlflow-tracing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff83d3d4-ad4a-49ee-9d19-9784db37ae58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note: this is for the demo only to parametrize the value with the right values (warehouse ID and catalog/schema). \n",
    "# In a real deployment, just put your configuration in the config.yaml file and skip this cell\n",
    "\n",
    "try:\n",
    "  wh = get_shared_warehouse(name = None)\n",
    "  config = {\n",
    "    \"agent_prompt\": \"\"\"Act as an assistant for wind turbine maintenance technicians.\\n\n",
    "    These are the tools you can use to answer questions:\n",
    "    \\n- turbine_maintenance_predictor: takes as input sensor_readings and predicts whether or not a turbine is at risk of failure.\n",
    "    \\n- turbine_maintenance_reports_predictor: takes sensor_readings as input and retrieves historical maintenance reports with similar sensor_readings. Critical for prescriptive maintenance.\n",
    "    \\n- turbine_specifications_retriever: takes turbine_id as input and retrieves turbine specifications.\n",
    "    \n",
    "\n",
    "    \\nIf a user gives you a turbine ID, first look up that turbine's information with turbine_specifications_retriever. \n",
    "    \\nIf a user asks for recommendations on how to do maintenance on a turbine, use the turbine reading and search for similar reports matching the turbine readings using the  turbine_maintenance_reports_predictor. Use the report retrived from other turbines to understand what could be happening and suggest maintenance recommendation.\n",
    "    \"\"\",\n",
    "    \"llm_endpoint\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    \"warehouse_id\": wh.id,\n",
    "    \"uc_functions\": [\n",
    "      f\"{catalog}.{schema}.turbine_specifications_retriever\",\n",
    "      f\"{catalog}.{schema}.turbine_maintenance_reports_retriever\",\n",
    "      f\"{catalog}.{schema}.turbine_maintenance_predictor\"\n",
    "    ]\n",
    "  }\n",
    "\n",
    "  import yaml\n",
    "  with open('config.yml', 'w') as f:\n",
    "      yaml.dump(config, f)\n",
    "except Exception as e:\n",
    "    print(\"Could not write the file - make sure it exists in the local folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a7e9bec-e554-46d5-9e69-efac80dde467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import ModelConfig\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "config = ModelConfig(development_config=\"config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe518529-1c6c-47d8-a393-2fe79901b56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the chat model and tools\n",
    "Create a LangChain chat model that supports [LangGraph tool](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/) calling.\n",
    "\n",
    "Modify the tools your agent has access to by modifying the `uc_functions` list in [config.yml]($./config.yml). Any non-UC function spec tools can be defined in this notebook. See [LangChain - How to create tools](https://python.langchain.com/v0.2/docs/how_to/custom_tools/) and [LangChain - Using built-in tools](https://python.langchain.com/v0.2/docs/how_to/tools_builtin/).\n",
    "\n",
    " **_NOTE:_**  This notebook uses LangChain, however AI Agent Framework is compatible with other agent frameworks like Pyfunc and LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9a42de-5efd-49d8-bc9d-034fcfd688f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.tools.databricks import UCFunctionToolkit\n",
    "\n",
    "# Create the llm\n",
    "llm = ChatDatabricks(endpoint=config.get(\"llm_endpoint\"))\n",
    "\n",
    "uc_functions = config.get(\"uc_functions\")\n",
    "\n",
    "tools = (\n",
    "    UCFunctionToolkit(warehouse_id=config.get(\"warehouse_id\"))\n",
    "    .include(*uc_functions)\n",
    "    .get_tools()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a68f88f4-9eac-400a-b94b-4e859000dbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Output parsers\n",
    "Databricks interfaces, such as the AI Playground, can optionally display pretty-printed tool calls.\n",
    "\n",
    "Use the following helper functions to parse the LLM's output into the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43d4f92f-2639-4560-bbb3-25e02fadaa7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Dict, Any\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    "    MessageLikeRepresentation,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "def stringify_tool_call(tool_call: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert a raw tool call into a formatted string that the playground UI expects if there is enough information in the tool_call\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = json.dumps(\n",
    "            {\n",
    "                \"id\": tool_call.get(\"id\"),\n",
    "                \"name\": tool_call.get(\"name\"),\n",
    "                \"arguments\": json.dumps(tool_call.get(\"args\", {})),\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "        return f\"<tool_call>{request}</tool_call>\"\n",
    "    except:\n",
    "        return str(tool_call)\n",
    "\n",
    "\n",
    "def stringify_tool_result(tool_msg: ToolMessage) -> str:\n",
    "    \"\"\"\n",
    "    Convert a ToolMessage into a formatted string that the playground UI expects if there is enough information in the ToolMessage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = json.dumps(\n",
    "            {\"id\": tool_msg.tool_call_id, \"content\": tool_msg.content}, indent=2\n",
    "        )\n",
    "        return f\"<tool_call_result>{result}</tool_call_result>\"\n",
    "    except:\n",
    "        return str(tool_msg)\n",
    "\n",
    "\n",
    "def parse_message(msg) -> str:\n",
    "    \"\"\"Parse different message types into their string representations\"\"\"\n",
    "    # tool call result\n",
    "    if isinstance(msg, ToolMessage):\n",
    "        return stringify_tool_result(msg)\n",
    "    # tool call\n",
    "    elif isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "        tool_call_results = [stringify_tool_call(call) for call in msg.tool_calls]\n",
    "        return \"\".join(tool_call_results)\n",
    "    # normal HumanMessage or AIMessage (reasoning or final answer)\n",
    "    elif isinstance(msg, (AIMessage, HumanMessage)):\n",
    "        return msg.content\n",
    "    else:\n",
    "        print(f\"Unexpected message type: {type(msg)}\")\n",
    "        return str(msg)\n",
    "\n",
    "\n",
    "def wrap_output(stream: Iterator[MessageLikeRepresentation]) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Process and yield formatted outputs from the message stream.\n",
    "    The invoke and stream langchain functions produce different output formats.\n",
    "    This function handles both cases.\n",
    "    \"\"\"\n",
    "    for event in stream:\n",
    "        # the agent was called with invoke()\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                yield parse_message(msg) + \"\\n\\n\"\n",
    "        # the agent was called with stream()\n",
    "        else:\n",
    "            for node in event:\n",
    "                for key, messages in event[node].items():\n",
    "                    if isinstance(messages, list):\n",
    "                        for msg in messages:\n",
    "                            yield parse_message(msg) + \"\\n\\n\"\n",
    "                    else:\n",
    "                        print(\"Unexpected value {messages} for key {key}. Expected a list of `MessageLikeRepresentation`'s\")\n",
    "                        yield str(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4e421b0-bb13-4e29-9e59-660a72e36436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create the agent\n",
    "Here we provide a simple graph that uses the model and tools defined by [config.yml]($./config.yml). This graph is adapated from [this LangGraph guide](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/).\n",
    "\n",
    "\n",
    "To further customize your LangGraph agent, you can refer to:\n",
    "* [LangGraph - Quick Start](https://langchain-ai.github.io/langgraph/tutorials/introduction/) for explanations of the concepts used in this LangGraph agent\n",
    "* [LangGraph - How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/) to expand the functionality of your agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65ef3872-faa6-4c2a-b7f8-86961ec5bfff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "\n",
    "# We create the AgentState that we will pass around\n",
    "# This simply involves a list of messages\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolExecutor, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there is no function call, then we finish\n",
    "        if not last_message.tool_calls:\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = SystemMessage(content=agent_prompt)\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    # Define the function that calls the model\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        # First, we define the start node. We use agent.\n",
    "        # This means these are the edges taken after the agent node is called.\n",
    "        \"agent\",\n",
    "        # Next, we pass in the function that will determine which node is called next.\n",
    "        should_continue,\n",
    "        # The mapping below will be used to determine which node to go to\n",
    "        {\n",
    "            # If tools, then we call the tool node.\n",
    "            \"continue\": \"tools\",\n",
    "            # END is a special node marking that the graph should finish.\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    # We now add a unconditional edge from tools to agent.\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3eb9f4e-3a18-4bf2-b476-fdfdee76fe86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "from mlflow.langchain.output_parsers import ChatCompletionsOutputParser\n",
    "\n",
    "# Create the agent with the system message if it exists\n",
    "try:\n",
    "    agent_prompt = config.get(\"agent_prompt\")\n",
    "    agent_with_raw_output = create_tool_calling_agent(\n",
    "        llm, tools, agent_prompt=agent_prompt\n",
    "    )\n",
    "except KeyError:\n",
    "    agent_with_raw_output = create_tool_calling_agent(llm, tools)\n",
    "agent = agent_with_raw_output | RunnableGenerator(wrap_output) | ChatCompletionsOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fea7ab72-6c91-4344-90fc-bbd510ba1b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. Since this notebook called `mlflow.langchain.autolog()` you can view the trace for each step the agent takes.\n",
    "\n",
    "Replace this placeholder input with an appropriate domain-specific example for your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d503021-75b1-42fc-b309-44b3621752a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"How is turbine 5ef39b37-7f89-b8c2-aff1-5e4c0453377d performing?\"}]}):\n",
    "    print(event, \"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90545008-0a28-43ad-87cf-433aa227cbac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"Fetch me information and readings for turbine 004a641f-e9e5-9fff-d421-1bf88319420b. Give me maintenance recommendation based on existing reports\"}]}):\n",
    "    print(event, \"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3f04f2-09cc-4dcf-8c2d-aed02d449cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "812111ac-fe58-4df9-a71a-9c9b0bc55d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "You can rerun the cells above to iterate and test the agent.\n",
    "\n",
    "Go to the [05.3-deploy-agent-iot-turbine-prescriptive-maintenance]($./05.3-deploy-agent-iot-turbine-prescriptive-maintenance) notebook to log, register, and deploy the agent."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05.2-build-agent-iot-turbine-prescriptive-maintenance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
