{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75751f16-31f5-402f-9f9e-bbd7c20ad318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingesting and transforming IOT sensors from Wind Turbinge using Delta Lake and Spark API\n",
    "\n",
    "<img style=\"float: right\" width=\"300px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-2.png\" />\n",
    "\n",
    "In this notebook, we'll show you an alternative to Delta Live Table: building an ingestion pipeline with the Spark API.\n",
    "\n",
    "As you'll see, this implementation is lower level than the Delta Live Table pipeline, and you'll have control over all the implementation details (handling checkpoints, data quality etc).\n",
    "\n",
    "Lower level also means more power. Using Spark API, you'll have unlimited capabilities to ingest data in Batch or Streaming.\n",
    "\n",
    "If you're unsure what to use, start with Delta Live Table!\n",
    "\n",
    "*Remember that Databricks workflow can be used to orchestrate a mix of Delta Live Table pipeline with standard Spark pipeline.*\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "As reminder, we have multiple data sources coming from different system:\n",
    "\n",
    "* <strong>Turbine metadata</strong>: Turbine ID, location (1 row per turbine)\n",
    "* <strong>Turbine sensor stream</strong>: Realtime streaming flow from wind turbine sensor (vibration, energy produced, speed etc)\n",
    "* <strong>Turbine status</strong>: Historical turbine status based to analyse which part is faulty (used as label in our ML model)\n",
    "\n",
    "\n",
    "Leveraging Spark and Delta Lake makes such an implementation easy.\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=1444828305810485&notebook=%2F01-Data-ingestion%2Fplain-spark-delta-pipeline%2F01.5-Delta-pipeline-spark-iot-turbine&demo_name=lakehouse-iot-platform&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-iot-platform%2F01-Data-ingestion%2Fplain-spark-delta-pipeline%2F01.5-Delta-pipeline-spark-iot-turbine&version=1&user_hash=285b361d244545c3c8d34d28625a163a3358e9bd9b085d42cefe76c0b086ca5d\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e00586d-820d-4f52-8580-7444dbeabc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-lakehouse-iot-platform-rishabh_shastry` from the dropdown menu ([open cluster configuration](https://e2-demo-field-eng.cloud.databricks.com/#setting/clusters/0430-201512-50kngnyt/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('lakehouse-iot-platform')` or re-install the demo: `dbdemos.install('lakehouse-iot-platform')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebbea978-25e4-47b3-8bd2-61dd367119fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==2.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09d3856f-f9d6-45d9-9f4a-995f18ebd1e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../../_resources/00-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74f7746f-0e9f-4991-8d08-ae2d60c84c03",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the version from our mlflow run"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "import os\n",
    "import mlflow\n",
    "# Use the Unity Catalog model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "# download model requirement from remote registry\n",
    "requirements_path = ModelsArtifactRepository(f\"models:/{catalog}.{db}.dbdemos_turbine_maintenance@prod\").download_artifacts(artifact_path=\"requirements.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d94abb2-7427-4933-be1e-61ebd995c1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r $requirements_path\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a58c0459-c76f-492a-8464-b2fb1ecbf1a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2084c33-0f50-4a89-b18d-8486ff3863a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building a Spark Data pipeline with Delta Lake\n",
    "\n",
    "In this example, we'll implement a end 2 end pipeline consuming our IOT sources. We'll use the medaillon architecture but could build a star schema, data vault or any other modelisation.\n",
    "\n",
    "\n",
    "\n",
    "This can be challenging with traditional systems due to the following:\n",
    " * Data quality issue\n",
    " * Running concurrent operation\n",
    " * Running DELETE/UPDATE/MERGE over files\n",
    " * Governance & schema evolution\n",
    " * Performance ingesting millions of small files on cloud buckets\n",
    " * Processing & analysing unstructured data (image, video...)\n",
    " * Switching between batch or streaming depending of your requirement...\n",
    "\n",
    "## Solving these challenges with Delta Lake\n",
    "\n",
    "<div style=\"float:left\">\n",
    "\n",
    "**What's Delta Lake? It's a new OSS standard to bring SQL Transactional database capabilities on top of parquet files!**\n",
    "\n",
    "Used as a new Spark format, built on top of Spark API / SQL\n",
    "\n",
    "* **ACID transactions** (Multiple writers can simultaneously modify a data set)\n",
    "* **Full DML support** (UPDATE/DELETE/MERGE)\n",
    "* **BATCH and STREAMING** support\n",
    "* **Data quality** (expectatiosn, Schema Enforcement, Inference and Evolution)\n",
    "* **TIME TRAVEL** (Look back on how data looked like in the past)\n",
    "* **Performance boost** with ZOrder, data skipping and Caching, solves small files issue \n",
    "</div>\n",
    "\n",
    "\n",
    "<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-logo.png\" style=\"height: 200px\"/>\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "We'll incrementally load new data with the autoloader, enrich this information and then load a model from MLFlow to perform our predictive maintenance forecast.\n",
    "\n",
    "This information will then be used to build our DBSQL dashboard to analyse current turbine farm and impact on stock.\n",
    "\n",
    "Let'simplement the following flow: \n",
    " \n",
    "<div><img width=\"1100px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-spark-full.png\"/></div>\n",
    "\n",
    "*Note that we're including the ML model our [Data Scientist built](TODO) using Databricks AutoML to predict the churn.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3107cc6e-144d-4447-b164-14eaa3e7a235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 1/ Explore the dataset\n",
    "\n",
    "Let's review the files being received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "742acacb-a7d0-48c3-9200-0f2c9c3e80b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql LIST '/Volumes/main/dbdemos_iot_turbine_team3_fs/turbine_raw_landing/incoming_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0c16f91-8943-480c-89d9-5c74adc6e9e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Review the raw sensor data received as JSON"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM PARQUET.`/Volumes/main/dbdemos_iot_turbine_team3_fs/turbine_raw_landing/incoming_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad94e0c-6309-4b99-94f3-3f8a7d73affa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1/ Loading our data using Databricks Autoloader (cloud_files)\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"700px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-spark-1.png\"/>\n",
    "</div>\n",
    "  \n",
    "Autoloader allow us to efficiently ingest millions of files from a cloud storage, and support efficient schema inference and evolution at scale.\n",
    "\n",
    "For more details on autoloader, run `dbdemos.install('auto-loader')`\n",
    "\n",
    "Let's use it to create our pipeline and ingest the raw JSON & CSV data being delivered in our blob storage `/demos/retail/churn/...`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e8a4b95-7413-4186-bc12-7023ef17dc08",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "We'll store the raw data in a USER_BRONZE DELTA table, supporting schema evolution and incorrect data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Note: tables are automatically created during  .writeStream.table(\"sensor_bronze\") operation, but we can also use plain SQL to create them:\n",
    "CREATE TABLE IF NOT EXISTS spark_sensor_bronze (\n",
    "  energy   DOUBLE,\n",
    "  sensor_A DOUBLE,\n",
    "  sensor_B DOUBLE,\n",
    "  sensor_C DOUBLE,\n",
    "  sensor_D DOUBLE,\n",
    "  sensor_E DOUBLE,\n",
    "  sensor_F DOUBLE,\n",
    "  timestamp LONG,\n",
    "  turbine_id STRING     \n",
    "  ) using delta \n",
    "    CLUSTER BY (turbine_id) -- Requests by turbine ID will be faster, Databricks manage the file layout for you out of the box. \n",
    "    TBLPROPERTIES (\n",
    "     delta.autooptimize.optimizewrite = TRUE,\n",
    "     delta.autooptimize.autocompact   = TRUE ); \n",
    "-- With these 2 last options, Databricks engine will solve small files & optimize write out of the box!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97655cd1-3eed-4cf0-81f9-4aacfbe394ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume_folder = f'/Volumes/{catalog}/{db}/{volume_name}'\n",
    "def ingest_folder(folder, data_format, table):\n",
    "  bronze_products = (spark.readStream\n",
    "                              .format(\"cloudFiles\")\n",
    "                              .option(\"cloudFiles.format\", data_format)\n",
    "                              .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                              .option(\"cloudFiles.schemaLocation\", f\"{volume_folder}/schema/{table}\") #Autoloader will automatically infer all the schema & evolution\n",
    "                              .load(folder))\n",
    "\n",
    "  return (bronze_products.writeStream\n",
    "                    .option(\"checkpointLocation\", f\"{volume_folder}/checkpoint/{table}\") #exactly once delivery on Delta tables over restart/kill\n",
    "                    .option(\"mergeSchema\", \"true\") #merge any new column dynamically\n",
    "                    .trigger(availableNow= True) #Remove for real time streaming\n",
    "                    .table(\"spark_\"+table)) #Table will be created if we haven't specified the schema first\n",
    "  \n",
    "ingest_folder(f'{volume_folder}/historical_turbine_status', 'json', 'historical_turbine_status')\n",
    "ingest_folder(f'{volume_folder}/turbine', 'json', 'turbine')\n",
    "ingest_folder(f'{volume_folder}/incoming_data', 'parquet', 'sensor_bronze').awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4231f716-f655-437b-9c05-9c6a809623f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Our user_bronze Delta table is now ready for efficient query"
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- Note the \"_rescued_data\" column. If we receive wrong data not matching existing schema, it'll be stored here\n",
    "select * from spark_sensor_bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4675446-3c1b-4c7d-b458-5b11b77e3e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- Note the \"_rescued_data\" column. If we receive wrong data not matching existing schema, it'll be stored here\n",
    "select * from spark_turbine;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d9857a-cf6e-42d9-befd-5a51a79c1b9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quick data exploration leveraging pandas on spark (Koalas): sensor from our first turbine"
    }
   },
   "outputs": [],
   "source": [
    "#Let's explore a bit our datasets with pandas on spark.\n",
    "first_turbine = spark.table('spark_sensor_bronze').limit(1).collect()[0]['turbine_id']\n",
    "df = spark.table('spark_sensor_bronze').where(f\"turbine_id == '{first_turbine}' \").orderBy('timestamp').pandas_api()\n",
    "df.plot(x=\"timestamp\", y=[\"sensor_F\", \"sensor_E\"], kind=\"line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64713efd-3b43-4649-8711-eb774eb95620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 2/ Silver data: date cleaned\n",
    "\n",
    "<img width=\"700px\" style=\"float:right\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-spark-2.png\"/>\n",
    "\n",
    "We can chain these incremental transformation between tables, consuming only new data.\n",
    "\n",
    "This can be triggered in near realtime, or in batch fashion, for example as a job running every night to consume daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a16d2c62-e124-4244-bd51-88d78146fca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#Compute std and percentil of our timeserie per hour\n",
    "sensors = [c for c in spark.read.table(\"spark_sensor_bronze\").columns if \"sensor\" in c]\n",
    "aggregations = [F.avg(\"energy\").alias(\"avg_energy\")]\n",
    "for sensor in sensors:\n",
    "  aggregations.append(F.stddev_pop(sensor).alias(\"std_\"+sensor))\n",
    "  aggregations.append(F.percentile_approx(sensor, [0.1, 0.3, 0.6, 0.8, 0.95]).alias(\"percentiles_\"+sensor))\n",
    "  \n",
    "df = (spark.table(\"spark_sensor_bronze\")\n",
    "          .withColumn(\"hourly_timestamp\", F.date_trunc(\"hour\", F.from_unixtime(\"timestamp\")))\n",
    "          .groupBy('hourly_timestamp', 'turbine_id').agg(*aggregations))\n",
    "\n",
    "df.write.mode('overwrite').saveAsTable(\"spark_sensor_hourly\")\n",
    "display(spark.table(\"spark_sensor_hourly\"))\n",
    "#Note: a more scalable solution would be to switch to streaming API and compute the aggregation with a ~3hours watermark and MERGE (upserting) the final output. For this demo clarity we we'll go with a full table update instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5dced6-9eaf-46a5-95bb-e4deecb0d783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 3/ Build our training dataset\n",
    "\n",
    "<img width=\"700px\" style=\"float:right\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-spark-3.png\"/>\n",
    "\n",
    "We can chain these incremental transformation between tables, consuming only new data.\n",
    "\n",
    "This can be triggered in near realtime, or in batch fashion, for example as a job running every night to consume daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90647495-e7e4-490f-b025-19be8afcc3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "turbine = spark.table(\"spark_turbine\")\n",
    "health = spark.table(\"spark_historical_turbine_status\")\n",
    "(spark.table(\"spark_sensor_hourly\")\n",
    "  .join(turbine, ['turbine_id']).drop(\"row\", \"_rescued_data\")\n",
    "  .join(health, ['turbine_id'])\n",
    "  .drop(\"_rescued_data\")\n",
    "  .write.mode('overwrite').saveAsTable(\"spark_turbine_training_dataset\"))\n",
    "\n",
    "display(spark.table(\"spark_turbine_training_dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce07e9ab-80ce-4ee8-926f-8b3360fd176c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 4/ Call the ML model and get realtime turbine metrics\n",
    "\n",
    "<img width=\"700px\" style=\"float:right\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-spark-4.png\"/>\n",
    "\n",
    "We can chain these incremental transformation between tables, consuming only new data.\n",
    "\n",
    "This can be triggered in near realtime, or in batch fashion, for example as a job running every night to consume daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fdcc9fa-660f-4c85-a8c0-4cfda364f1c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the ML model"
    }
   },
   "outputs": [],
   "source": [
    "#Note: ideally we should download and install the model libraries with the model requirements.txt and PIP. See 04.3-running-inference for an example\n",
    "import mlflow\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "#                                                                                                                       Stage/version  \n",
    "#                                                                                                       Model name         |        \n",
    "#                                                                                                           |              |        \n",
    "predict_maintenance = mlflow.pyfunc.spark_udf(spark, f\"models:/{catalog}.{db}.dbdemos_turbine_maintenance@prod\", \"string\") #, env_manager='virtualenv'\n",
    "columns = predict_maintenance.metadata.get_input_schema().input_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6545d9c7-1cf8-409d-b01f-64f645ac52be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"turbine_id\").orderBy(col(\"hourly_timestamp\").desc())\n",
    "(spark.table(\"spark_sensor_hourly\")\n",
    "  .withColumn(\"row\", F.row_number().over(w))\n",
    "  .filter(col(\"row\") == 1)\n",
    "  .join(spark.table('spark_turbine'), ['turbine_id']).drop(\"row\", \"_rescued_data\")\n",
    "  .withColumn(\"prediction\", predict_maintenance(*columns))\n",
    "  .write.mode('overwrite').saveAsTable(\"spark_current_turbine_metrics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c02c5ece-5f69-49e6-85f3-ba72639481dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from spark_current_turbine_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a8fde86-42e8-47b3-b174-0e770030fbb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simplify your operations with transactional DELETE/UPDATE/MERGE operations\n",
    "\n",
    "Traditional Data Lake struggle to run these simple DML operations. Using Databricks and Delta Lake, your data is stored on your blob storage with transactional capabilities. You can issue DML operation on Petabyte of data without having to worry about concurrent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22d22dd-a6d7-4eb8-9e65-f04af480a737",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "We just realised we have to delete bad entry for a specific turbine"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DELETE FROM spark_sensor_bronze where turbine_id='{first_turbine}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0408452-eb35-47e2-9b01-8c2d1c090941",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delta Lake keeps history of the table operation"
    }
   },
   "outputs": [],
   "source": [
    "%sql describe history spark_sensor_bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "598b964e-1ca5-413f-a6d8-6c040dfe36e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "We can leverage the history to go back in time, restore or clone a table and enable CDC..."
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    " --also works with AS OF TIMESTAMP \"yyyy-MM-dd HH:mm:ss\"\n",
    "select * from spark_sensor_bronze version as of 1 ;\n",
    "\n",
    "-- You made the DELETE by mistake ? You can easily restore the table at a given version / date:\n",
    "-- RESTORE TABLE spark_sensor_bronze TO VERSION AS OF 1\n",
    "\n",
    "-- Or clone it (SHALLOW provides zero copy clone):\n",
    "-- CREATE TABLE spark_sensor_bronze_clone SHALLOW|DEEP CLONE sensor_bronze VERSION AS OF 1\n",
    "\n",
    "-- Turn on CDC to capture insert/update/delete operation:\n",
    "-- ALTER TABLE spark_sensor_bronze SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf2518c-9990-43b8-a10d-1c5a13863666",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Make sure all our tables are optimized"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Note: can be turned on by default or for all the database\n",
    "ALTER TABLE spark_turbine                  SET TBLPROPERTIES (delta.autooptimize.optimizewrite = TRUE, delta.autooptimize.autocompact = TRUE );\n",
    "ALTER TABLE spark_sensor_bronze            SET TBLPROPERTIES (delta.autooptimize.optimizewrite = TRUE, delta.autooptimize.autocompact = TRUE );\n",
    "ALTER TABLE spark_current_turbine_metrics  SET TBLPROPERTIES (delta.autooptimize.optimizewrite = TRUE, delta.autooptimize.autocompact = TRUE );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c67d0f78-3a39-40b9-966f-5ad3909bf610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Our finale tables are now ready to be used to build SQL Dashboards and ML models for predictive maintenance!\n",
    "<img style=\"float: right\" width=\"400\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-dashboard-1.png\"/>\n",
    "\n",
    "Switch to Databricks SQL to see how this data can easily be requested with the [Turbine DBSQL Dashboard](/sql/dashboards/a6bb11d9-1024-47df-918d-f47edc92d5f4) to start reviewing our Wind Turbine stats or the [DBSQL Predictive maintenance Dashboard](/sql/dashboards/d966eb63-6d37-4762-b90f-d3a2b51b9ba8).\n",
    "\n",
    "Creating a single flow was simple.  However, handling many data pipeline at scale can become a real challenge:\n",
    "* Hard to build and maintain table dependencies \n",
    "* Difficult to monitor & enforce advance data quality\n",
    "* Impossible to trace data lineage\n",
    "* Difficult pipeline operations (observability, error recovery)\n",
    "\n",
    "\n",
    "#### To solve these challenges, Databricks introduced **Delta Live Table**\n",
    "A simple way to build and manage data pipelines for fresh, high quality data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10399d85-38fc-4138-9bc5-84066bcc8ad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Next: secure and share data with Unity Catalog\n",
    "\n",
    "Now that these tables are available in our Lakehouse, let's review how we can share them with the Data Scientists and Data Analysts teams.\n",
    "\n",
    "Jump to the [Governance with Unity Catalog notebook]($../../02-Data-governance/02-UC-data-governance-security-iot-turbine) or [Go back to the introduction]($../../00-IOT-wind-turbine-introduction-lakehouse)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01.5-Delta-pipeline-spark-iot-turbine",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
