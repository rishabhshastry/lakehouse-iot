{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67458cd-5dd0-4cd6-bef2-b9cd933417ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data engineering with Databricks - Building our Manufacturing IOT platform\n",
    "\n",
    "Building an IOT platform requires to ingest multiple datasources.  \n",
    "\n",
    "It's a complex process requiring batch loads and streaming ingestion to support real-time insights, used for real-time monitoring among others.\n",
    "\n",
    "Ingesting, transforming and cleaning data to create clean SQL tables for our downstream user (Data Analysts and Data Scientists) is complex.\n",
    "\n",
    "<link href=\"https://fonts.googleapis.com/css?family=DM Sans\" rel=\"stylesheet\"/>\n",
    "<div style=\"width:300px; text-align: center; float: right; margin: 30px 60px 10px 10px;  font-family: 'DM Sans'\">\n",
    "  <div style=\"height: 300px; width: 300px;  display: table-cell; vertical-align: middle; border-radius: 50%; border: 25px solid #fcba33ff;\">\n",
    "    <div style=\"font-size: 70px;  color: #70c4ab; font-weight: bold\">\n",
    "      73%\n",
    "    </div>\n",
    "    <div style=\"color: #1b5162;padding: 0px 30px 0px 30px;\">of enterprise data goes unused for analytics and decision making</div>\n",
    "  </div>\n",
    "  <div style=\"color: #bfbfbf; padding-top: 5px\">Source: Forrester</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "## <img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/john.png\" style=\"float:left; margin: -35px 0px 0px 0px\" width=\"80px\"> John, as Data engineer, spends immense timeâ€¦.\n",
    "\n",
    "\n",
    "* Hand-coding data ingestion & transformations and dealing with technical challenges:<br>\n",
    "  *Supporting streaming and batch, handling concurrent operations, small files issues, GDPR requirements, complex DAG dependencies...*<br><br>\n",
    "* Building custom frameworks to enforce quality and tests<br><br>\n",
    "* Building and maintaining scalable infrastructure, with observability and monitoring<br><br>\n",
    "* Managing incompatible governance models from different systems\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "This results in **operational complexity** and overhead, requiring expert profile and ultimatly **putting data projects at risk**.\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=1444828305810485&notebook=%2F01-Data-ingestion%2F01.1-DLT-Wind-Turbine-SQL&demo_name=lakehouse-iot-platform&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-iot-platform%2F01-Data-ingestion%2F01.1-DLT-Wind-Turbine-SQL&version=1&user_hash=285b361d244545c3c8d34d28625a163a3358e9bd9b085d42cefe76c0b086ca5d\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98d3f0e1-653e-41c0-837c-60480a619dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simplify Ingestion and Transformation with Delta Live Tables\n",
    "\n",
    "<img style=\"float: right\" width=\"500px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/team_flow_john.png\" />\n",
    "\n",
    "In this notebook, we'll work as a Data Engineer to build our IOT platform. <br>\n",
    "We'll ingest and clean our raw data sources to prepare the tables required for our BI & ML workload.\n",
    "\n",
    "Databricks simplifies this task with Delta Live Table (DLT) by making Data Engineering accessible to all.\n",
    "\n",
    "DLT allows Data Analysts to create advanced pipeline with plain SQL.\n",
    "\n",
    "## Delta Live Table: A simple way to build and manage data pipelines for fresh, high quality data!\n",
    "\n",
    "<div>\n",
    "  <div style=\"width: 45%; float: left; margin-bottom: 10px; padding-right: 45px\">\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-accelerate.png\"/> \n",
    "      <strong>Accelerate ETL development</strong> <br/>\n",
    "      Enable analysts and data engineers to innovate rapidly with simple pipeline development and maintenance \n",
    "    </p>\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-complexity.png\"/> \n",
    "      <strong>Remove operational complexity</strong> <br/>\n",
    "      By automating complex administrative tasks and gaining broader visibility into pipeline operations\n",
    "    </p>\n",
    "  </div>\n",
    "  <div style=\"width: 48%; float: left\">\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-trust.png\"/> \n",
    "      <strong>Trust your data</strong> <br/>\n",
    "      With built-in quality controls and quality monitoring to ensure accurate and useful BI, Data Science, and ML \n",
    "    </p>\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-stream.png\"/> \n",
    "      <strong>Simplify batch and streaming</strong> <br/>\n",
    "      With self-optimization and auto-scaling data pipelines for batch or streaming processing \n",
    "    </p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\">\n",
    "\n",
    "<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-logo.png\" style=\"float: right;\" width=\"200px\">\n",
    "\n",
    "## Delta Lake\n",
    "\n",
    "All the tables we'll create in the Data Intelligence Platform will be stored as Delta Lake table. Delta Lake is an open storage framework for reliability and performance.<br>\n",
    "It provides many functionalities (ACID Transaction, DELETE/UPDATE/MERGE, Clone zero copy, Change data Capture...)<br>\n",
    "For more details on Delta Lake, run dbdemos.install('delta-lake')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ba3216a-335b-4b79-b874-0618e50d14f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building a Delta Live Table pipeline to ingest IOT sensor and detect faulty equipments\n",
    "\n",
    "In this example, we'll implement a end 2 end DLT pipeline consuming our Wind Turbine sensor data. <br/>\n",
    "We'll use the medaillon architecture but we could build star schema, data vault or any other modelisation.\n",
    "\n",
    "We'll incrementally load new data with the autoloader, enrich this information and then load a model from MLFlow to perform our predictive maintenance analysis.\n",
    "\n",
    "This information will then be used to build our AI/BI dashboards to track our wind turbine farm status, faulty equipment impact and recommendations to reduce potential downtime.\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "* <strong>Turbine metadata</strong>: Turbine ID, location (1 row per turbine)\n",
    "* <strong>Turbine sensor stream</strong>: Realtime streaming flow from wind turbine sensor (vibration, energy produced, speed etc)\n",
    "* <strong>Turbine status</strong>: Historical turbine status based to analyse which part is faulty (used as label in our ML model)\n",
    "\n",
    "\n",
    "Let's implement the following flow: \n",
    " \n",
    "<div><img width=\"1100px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-full.png\"/></div>\n",
    "\n",
    "*Note that we're including the ML model our [Data Scientist built]($../04-Data-Science-ML/04.1-automl-predictive-maintenance-turbine) using Databricks AutoML to predict the churn. We'll cover that in the next section.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d3d4c15-0c62-475a-8378-acef5dbecee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Your DLT Pipeline has been installed and started for you! Open the <a dbdemos-pipeline-id=\"dlt-iot-wind-turbine\" href=\"#joblist/pipelines/e6653220-a089-4b36-9474-c84a5328706d\" target=\"_blank\">IOT Wind Turbine Delta Live Table pipeline</a> to see it in action.<br/>\n",
    "\n",
    "*(Note: The pipeline will automatically start once the initialization job is completed with dbdemos, this might take a few minutes... Check installation logs for more details)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "628583ec-c2ec-4bb4-b426-8fe5724e81a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wind Turbine metadata:"
    }
   },
   "outputs": [],
   "source": [
    "-- %python #uncomment to scan the data from the notebook\n",
    "-- display(spark.read.json('/Volumes/main/dbdemos_iot_turbine_team3_fs/turbine_raw_landing/turbine'))\n",
    "-- display(spark.read.json('/Volumes/main/dbdemos_iot_turbine_team3_fs/turbine_raw_landing/historical_turbine_status')) #Historical turbine status analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa9742d0-4ee5-4dc1-b5eb-41f2c1baf4b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wind Turbine sensor data"
    }
   },
   "outputs": [],
   "source": [
    "-- %python #uncomment to scan the data from the notebook\n",
    "-- display(spark.read.parquet('/Volumes/main/dbdemos_iot_turbine_team3_fs/turbine_raw_landing/incoming_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2c47f60-4cc1-4681-b050-f6ae523dea5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1/ Ingest data: ingest data using Auto Loader (cloudFiles)\n",
    "\n",
    "<div><img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-1.png\" width=\"700px\" style=\"float: right\"/></div>\n",
    "\n",
    "Ingesting data from stream source can be challenging. In this example we'll incrementally load the files from our cloud storage, only getting the new one (in near real-time or triggered every X hours).\n",
    "\n",
    "Note that while our streaming data is added to our cloud storage, we could easily ingest from kafka directly : `.format(kafka)`\n",
    "\n",
    "Auto-loader provides for you:\n",
    "\n",
    "- Schema inference and evolution\n",
    "- Scalability handling million of files\n",
    "- Simplicity: just define your ingestion folder, Databricks take care of the rest!\n",
    "\n",
    "For more details on autoloader, run `dbdemos.install('auto-loader')`\n",
    "\n",
    "Let's use it to our pipeline and ingest the raw JSON & CSV data being delivered in our blob storage `/demos/manufacturing/iot_turbine/...`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e404af3-deff-441f-b5cd-a3d2e7dc5ed6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Turbine metadata"
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING TABLE turbine (\n",
    "  CONSTRAINT correct_schema EXPECT (_rescued_data IS NULL)\n",
    ")\n",
    "COMMENT \"Turbine details, with location, wind turbine model type etc\"\n",
    "AS SELECT * FROM cloud_files(\"/Volumes/main/dbdemos_iot_turbine_team3_fs/turbine_raw_landing/turbine\", \"json\", map(\"cloudFiles.inferColumnTypes\" , \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e7550f-f042-472b-9622-ff5083d4a8c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wind Turbine sensor "
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING TABLE sensor_bronze (\n",
    "  CONSTRAINT correct_schema EXPECT (_rescued_data IS NULL),\n",
    "  CONSTRAINT correct_energy EXPECT (energy IS NOT NULL and energy > 0) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"Raw sensor data coming from json files ingested in incremental with Auto Loader: vibration, energy produced etc. 1 point every X sec per sensor.\"\n",
    "AS SELECT * FROM cloud_files(\"/Volumes/main/dbdemos_iot_turbine_team3_fs/turbine_raw_landing/incoming_data\", \"parquet\", map(\"cloudFiles.inferColumnTypes\" , \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0de9483-2d21-450e-9e21-548e43fa5127",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Historical status"
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING TABLE historical_turbine_status (\n",
    "  CONSTRAINT correct_schema EXPECT (_rescued_data IS NULL)\n",
    ")\n",
    "COMMENT \"Turbine status to be used as label in our predictive maintenance model (to know which turbine is potentially faulty)\"\n",
    "AS SELECT * FROM cloud_files(\"/Volumes/main/dbdemos_iot_turbine_team3_fs/turbine_raw_landing/historical_turbine_status\", \"json\", map(\"cloudFiles.inferColumnTypes\" , \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb8127f1-6268-4d51-9199-8377aea2423b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING TABLE parts \n",
    "COMMENT \"Turbine parts from our manufacturing system\"\n",
    "AS SELECT * FROM cloud_files(\"/Volumes/main/dbdemos_iot_turbine_team3_fs/turbine_raw_landing/parts\", \"json\", map(\"cloudFiles.inferColumnTypes\" , \"true\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7f31cfd-b228-4230-85b1-1721841cb09e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2/ Compute aggregations: merge sensor data at an hourly level\n",
    "\n",
    "<div><img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-2.png\" width=\"700px\" style=\"float: right\"/></div>\n",
    "\n",
    "To be able to analyze our data, we'll compute statistical metrics every at an ourly basis, such as standard deviation and quartiles.\n",
    "\n",
    "*Note that we'll be recomputing all the table to keep this example simple. We could instead UPSERT the current hour with a stateful agregation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fab9cce-2488-4f78-a94b-8d4739de32de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE MATERIALIZED VIEW sensor_hourly (\n",
    "  CONSTRAINT turbine_id_valid EXPECT (turbine_id IS not NULL)  ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT timestamp_valid EXPECT (hourly_timestamp IS not NULL)  ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"Hourly sensor stats, used to describe signal and detect anomalies\"\n",
    "AS\n",
    "SELECT turbine_id,\n",
    "      date_trunc('hour', from_unixtime(timestamp)) AS hourly_timestamp, \n",
    "      avg(energy)          as avg_energy,\n",
    "      stddev_pop(sensor_A) as std_sensor_A,\n",
    "      stddev_pop(sensor_B) as std_sensor_B,\n",
    "      stddev_pop(sensor_C) as std_sensor_C,\n",
    "      stddev_pop(sensor_D) as std_sensor_D,\n",
    "      stddev_pop(sensor_E) as std_sensor_E,\n",
    "      stddev_pop(sensor_F) as std_sensor_F,\n",
    "      percentile_approx(sensor_A, array(0.1, 0.3, 0.6, 0.8, 0.95)) as percentiles_sensor_A,\n",
    "      percentile_approx(sensor_B, array(0.1, 0.3, 0.6, 0.8, 0.95)) as percentiles_sensor_B,\n",
    "      percentile_approx(sensor_C, array(0.1, 0.3, 0.6, 0.8, 0.95)) as percentiles_sensor_C,\n",
    "      percentile_approx(sensor_D, array(0.1, 0.3, 0.6, 0.8, 0.95)) as percentiles_sensor_D,\n",
    "      percentile_approx(sensor_E, array(0.1, 0.3, 0.6, 0.8, 0.95)) as percentiles_sensor_E,\n",
    "      percentile_approx(sensor_F, array(0.1, 0.3, 0.6, 0.8, 0.95)) as percentiles_sensor_F\n",
    "  FROM LIVE.sensor_bronze GROUP BY hourly_timestamp, turbine_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ac27164-7bbc-4fd2-813e-a64485f15f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3/ Build our table used by ML Engineers: join sensor aggregates with wind turbine metadata and historical status\n",
    "\n",
    "<div><img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-3.png\" width=\"700px\" style=\"float: right\"/></div>\n",
    "\n",
    "Next, we'll build a final table joining sensor aggregate with our turbine information.\n",
    "\n",
    "This table will contain all the data required to be able to infer potential turbine failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e0b4bf-a870-4e57-8cb5-d7a42d75dfc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE MATERIALIZED VIEW turbine_training_dataset \n",
    "COMMENT \"Hourly sensor stats, used to describe signal and detect anomalies\"\n",
    "AS\n",
    "SELECT CONCAT(t.turbine_id, '-', s.start_time) AS composite_key, array(std_sensor_A, std_sensor_B, std_sensor_C, std_sensor_D, std_sensor_E, std_sensor_F) AS sensor_vector, * except(t._rescued_data, s._rescued_data, m.turbine_id) FROM LIVE.sensor_hourly m\n",
    "    INNER JOIN LIVE.turbine t USING (turbine_id)\n",
    "    INNER JOIN LIVE.historical_turbine_status s ON m.turbine_id = s.turbine_id AND from_unixtime(s.start_time) < m.hourly_timestamp AND from_unixtime(s.end_time) > m.hourly_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04e99649-cf36-4fd3-952f-6759f2e5a3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 4/ Get model from registry and add flag faulty turbines\n",
    "\n",
    "<div><img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-turbine-4.png\" width=\"700px\" style=\"float: right\"/></div>\n",
    "\n",
    "Our Data scientist team has been able to read data from our previous table and build a predictive maintenance model using Auto ML and saved it into Databricks Model registry (we'll see how to do that next).\n",
    "\n",
    "One of the key value of the Lakehouse is that we can easily load this model and predict faulty turbines with into our pipeline directly.\n",
    "\n",
    "Note that we don't have to worry about the model framework (sklearn or other), MLFlow abstract that for us.\n",
    "\n",
    "All we have to do is load the model, and call it as a SQL function (or python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888b90b8-1c55-4422-8b21-72a70d4e2549",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's create an intermediate feature table"
    }
   },
   "outputs": [],
   "source": [
    "-- specify all the field to enforce the primary key\n",
    "CREATE MATERIALIZED VIEW turbine_current_features\n",
    " (\n",
    "    turbine_id STRING NOT NULL,\n",
    "    hourly_timestamp TIMESTAMP,\n",
    "    avg_energy DOUBLE,\n",
    "    std_sensor_A DOUBLE,\n",
    "    std_sensor_B DOUBLE,\n",
    "    std_sensor_C DOUBLE,\n",
    "    std_sensor_D DOUBLE,\n",
    "    std_sensor_E DOUBLE,\n",
    "    std_sensor_F DOUBLE,\n",
    "    country STRING,\n",
    "    lat STRING,\n",
    "    location STRING,\n",
    "    long STRING,\n",
    "    model STRING,\n",
    "    state STRING,\n",
    "   CONSTRAINT turbine_current_features_pk PRIMARY KEY (turbine_id))\n",
    "COMMENT \"Wind turbine features based on model prediction\"\n",
    "AS\n",
    "WITH latest_metrics AS (\n",
    "  SELECT *, ROW_NUMBER() OVER(PARTITION BY turbine_id, hourly_timestamp ORDER BY hourly_timestamp DESC) AS row_number FROM LIVE.sensor_hourly\n",
    ")\n",
    "SELECT * EXCEPT(m.row_number,_rescued_data, percentiles_sensor_A,percentiles_sensor_B, percentiles_sensor_C, percentiles_sensor_D, percentiles_sensor_E, percentiles_sensor_F) \n",
    "FROM latest_metrics m\n",
    "   INNER JOIN LIVE.turbine t USING (turbine_id)\n",
    "   WHERE m.row_number=1 and turbine_id is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6849b625-fd74-4979-bd06-6ff18e82969f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Note: The AI model predict_maintenance is loaded from the 01.2-DLT-Wind-Turbine-SQL-UDF notebook\n",
    "CREATE MATERIALIZED VIEW turbine_current_status \n",
    "COMMENT \"Wind turbine last status based on model prediction\"\n",
    "AS\n",
    "SELECT *, \n",
    "    predict_maintenance(hourly_timestamp, avg_energy, std_sensor_A, std_sensor_B, std_sensor_C, std_sensor_D, std_sensor_E, std_sensor_F, location, model, state) as prediction \n",
    "  FROM LIVE.turbine_current_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e340da28-3d8c-4d5e-9514-4c8f4078733d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "Our <a dbdemos-pipeline-id=\"dlt-iot-wind-turbine\" href=\"#joblist/pipelines/e6653220-a089-4b36-9474-c84a5328706d\" target=\"_blank\">DLT Data Pipeline</a> is now ready using purely SQL. We have an end 2 end cycle, and our ML model has been integrated seamlessly by our Data Engineering team.\n",
    "\n",
    "\n",
    "For more details on model training, open the [model training notebook]($../04-Data-Science-ML/04.1-automl-iot-turbine-predictive-maintenance)\n",
    "\n",
    "Our final dataset includes our ML prediction for our Predictive Maintenance use-case. \n",
    "\n",
    "We are now ready to build our <a dbdemos-dashboard-id=\"turbine-analysis\" href=\"/sql/dashboardsv3/01f02600455d1bdaaeb15ed8fab3e23f\">AI/BI Dashboard</a> to track the main KPIs and status of our entire Wind Turbine Farm and build complete <a dbdemos-dashboard-id=\"turbine-predictive\" href=\"/sql/dashboardsv3/01f02600455d1bdaaeb15ed8fab3e23f\">Predictive maintenance AI/BI Dashboard</a>.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-dashboard-1.png\" width=\"1000px\">"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01.1-DLT-Wind-Turbine-SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
